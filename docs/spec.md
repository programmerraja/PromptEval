üß≠ PRODUCT SPECIFICATION DOCUMENT Project Title: PromptEval Tagline: Frontend-only LLM Prompt Evaluation, Auto-Eval, and Style Simulation Platform

üéØ Vision and Goals PromptEval is a frontend-only web application for prompt engineers, researchers, and developers who need to:
Rapidly test, compare, and evaluate prompts across models. Automate evaluation using LLM-vs-LLM judging (auto-eval). Test multi-turn conversations (dialogue scenarios). Extract and reuse user communication styles for simulation. Store and manage all data locally in the browser (no backend). Organize prompts, datasets, and evaluations by folders and tags. Everything ‚Äî prompts, configs, chat histories, datasets, and evaluation results ‚Äî is stored in IndexedDB / LocalStorage and can be exported/imported as JSON as single JSON file.

üß© System Architecture 2.1 Architecture Overview Frontend-only (SPA) architecture built with:
Framework: React + Vite State Management: Zustand UI Components: TailwindCSS + shadcn/ui Local Persistence: IndexedDB (via Dexie.js) Visualization: Recharts API Layer: OpenAI / Anthropic / Others via user-supplied keys use https://ai-sdk.dev/ No backend server. All interactions with APIs are direct from browser ‚Üí model endpoint using user‚Äôs API key.

2.2 Data Persistence Strategy Data Type Storage Layer Notes Prompt versions, conversations, datasets, eval results IndexedDB Structured, queryable API keys, theme, preferences LocalStorage Scoped to browser Export/Import JSON File For cross-device portability 3. üß± Core Modules 3.1 Dashboard (Home Page) Purpose: Quick visual overview and actionable insights.

Features:

Prompt Summary Table: | Prompt | Versions | Avg Score | Best Model | Last Eval | |--------|----------|-----------|------------|-----------| | BankingBot | 3 | 4.5 | gpt-4o-mini | 2d ago |

Quick Actions:

‚ûï New Prompt ‚ñ∂Ô∏è Run Eval üì¶ Import Dataset Auto-save indicator (top-right spinner + check mark when data is persisted locally)

Restore previous session button on startup

State diffs visible before export (highlight changed objects)

UX Note: Dashboard is read-only, action-driven ‚Äî no editing here.

3.2 Prompts Page Purpose: Manage and test prompt versions with integrated workspace.

Left Pane: List of prompts ‚Üí show name + version count + last modified Clicking expands a dropdown of versions (v1, v2, etc.) Folder organization and tagging system Right Workspace Tabs: Tab Description üß© Editor System + user prompt editing area with collapsible JSON config for parameters üí¨ Playground Full chat interface ‚Äî where user can chat and test the prompt conversation log, "Add to Dataset" button ü§ñ Auto Chat (LLM-vs-LLM) Define "how user behaves by choosing dataset" and run two-model conversation inline (Mini Multi-Chat) üìà Eval Results Displays past eval scores for this prompt (uses eval_results), with quick compare and "Run Eval" CTA linking to Eval page Action Buttons: üíæ Save Changes / Save as New Version üîÑ Clone Version UX Flow: User can stay within one prompt ‚Üí test ‚Üí eval ‚Üí save dataset without context switching.

3.3 Prompt Playground (Integrated) Purpose: Allow users to test prompts interactively within the Prompts page.

Features:

Input field for prompt Model selector (e.g. GPT-4, Claude, Mistral, etc.) Config panel (temperature, max tokens, system prompt, top_p, etc.) Output display (streaming supported) where user can chat with the model and see the conversation Token & latency display "Save as dataset entry" and "Save as new version" buttons Playground Sessions - temporary chat sessions that can be saved as conversations or datasets Playground Session Schema:

{ "id": "playground_001", "prompt_id": "prompt_001", "prompt_version": "v3", "model": "gpt-4o-mini", "config": { "temperature": 0.5, "max_tokens": 700, "top_p": 0.7 }, "messages": [ { "role": "user", "content": "I need help with my subscription billing" }, { "role": "assistant", "content": "I'd be happy to help you with your subscription billing..." } ], "usage": { "prompt_tokens": 45, "completion_tokens": 67, "total_tokens": 112, "latency": 1.8, "cost_estimate": 0.0003 }, "timestamp": "2025-01-15T15:00:00Z", "saved_as_version": false, "saved_as_dataset": false } 3.4 Eval Page Purpose: Configure and run evaluations with clear functional grouping.

Top Config Section: Dataset selector with preview modal (click to view sample entries) Model selector dropdown (show parameters inline) Prompt selector (show prompt preview) Tabs Flow: ‚öôÔ∏è Config: Choose judge prompt, eval criteria (checkboxes: "accuracy", "empathy", etc.) ‚ñ∂Ô∏è Run: Progress visual (animated status, token count) üìä Results: Interactive table ‚Äî sortable, filterable, exportable (CSV/JSON) UX Tips: After run ‚Üí auto-scroll to "Results" tab, show toast notification ("Evaluation complete ‚Äî score: 4.7") Add compare eval runs toggle to visualize performance across prompts or datasets 3.5 Dataset Page Purpose: Create and manage reusable test cases (single or multi-turn).

Left Panel: Filters: [Single-Turn | Multi-Turn], Tags, Linked Prompt Add search bar for quick entry lookup Folder organization and tagging system Right Panel: Render messages in chat-style UI Editable metadata: context, eval notes, tags Buttons: ‚úçÔ∏è Edit ‚ûï Add New Entry üóëÔ∏è Delete Entry üß† Generate "User Behavior Summary" üß™ Send to Eval Subtab: Multi-Turn Mode: Replay chat visually (like chat history scroll) Buttons: ‚ñ∂Ô∏è "Simulate with LLM-vs-LLM" üíæ "Add Conversation to Dataset" üóëÔ∏è Delete, ‚úèÔ∏è Edit, üîó Link to Prompt UX Tip: Use accordion collapses for long chats, with "expand all" option.

3.6 Multi-Chat Page (Conversation Simulator Lab) Purpose: Simulate LLM-vs-LLM conversations for automated testing.

Top Config Bar: Choose: LLM-A (User model) LLM-B (Assistant model) Dataset (for user behavior) Prompt version Temperature / Tokens Add Preview Dataset Behavior modal ‚Üí shows example user sentences before simulation Main Area: Split chat view: Left: LLM-A (Simulated user) Right: LLM-B (Assistant under test) Each message card shows role + model name + latency + tokens Bottom Bar: ‚ñ∂Ô∏è Play / ‚è∏ Pause / ‚è≠ Step üíæ Save Conversation ‚Üí Dataset üß™ Send to Eval üß† Export Transcript (JSON) UX Focus: Use color coding for sides (e.g., blue for Assistant, gray for User) Add small token and cost summary below chat after each turn 3.7 Settings Page Purpose: Configure application settings and preferences.

Tab Structure: General: Theme, auto-save settings, session management Dataset Generator: Model config for behavior extraction Evaluator: Default evaluation models and prompts Storage: Local storage usage, backup settings, reset options Features: Display estimated local storage usage (MB) Include reset confirmations ("This will delete all local data ‚Äî continue?") Add backup auto-export toggle (e.g., "Auto-export every 10 min") 3.8 Dataset Entry Types Single-turn Dataset Entry:

{ "id": "ds_002_entry_001", "type": "single-turn", "input": "Explain the concept of inflation in simple terms for a general audience.", "expected_behavior": "Clear, concise explanation using simple language and relatable examples.", "created_at": "2025-01-15T11:00:00Z" } Multi-turn Dataset Entry:

{ "id": "ds_001_entry_001", "type": "multi-turn", "title": "Password Reset Scenario", "system_context": "The assistant is a helpful tech support agent for a SaaS product.", "user_behavior": { "style": "Conversational, frustrated but polite", "formality": "semi-formal", "goal": "user forget his password and assistant help him to reset his password his email is test@test.com", "data": { "email": "test@test.com", "name": "John Doe" } }, "conversation": [ { "role": "user", "content": "Hi, I'm having trouble logging in." } ], "created_at": "2025-01-15T10:30:00Z" } Features:

Create / edit datasets via UI form or JSON editor Tag datasets (e.g. "summarization", "tone test", etc.) Group dataset entries under named test suites Folder organization and tagging system 3.9 Conversation System Purpose: Enable simulation, replay, and automated evaluation of multi-turn dialogues between:

Real user ‚Üî Model (manual testing mode) Simulated user (LLM) ‚Üî Tested model (auto evaluation mode) Conversation Types: Manual Conversations:

User selects model and prompt version Chats naturally with the model through a chat interface Every exchange is appended to the messages array User can mark the conversation as "complete" and add to dataset Auto-Eval Conversations:

Automatically simulate a conversation between two LLMs Model A (User Emulator): Generated from the dataset's "user behavior profile" Model B (Prompt Under Test): The candidate model version being evaluated Conversation Schema: { "id": "conv_001", "prompt_id": "prompt_001", "prompt_version": "v2", "model": "gpt-4o-mini", "type": "manual", "messages": [ { "role": "user", "content": "Hi, I'm having trouble logging in." }, { "role": "assistant", "content": "I'm sorry to hear you're having trouble logging in. I'd be happy to help you resolve this. What specific error message are you seeing when you try to log in?" }, { "role": "user", "content": "It says 'invalid credentials' but I know my password is correct." }, { "role": "assistant", "content": "I understand your frustration. Let's troubleshoot this step by step. First, could you please try resetting your password? I can send you a password reset link to your registered email address. What email address do you have on file with us?" }, { "role": "user", "content": "my email is john.doe@example.com" }, { "role": "assistant", "content": "Perfect! I've sent a password reset link to john.doe@example.com. Please check your email (including spam folder) and click the link to reset your password. Once you've reset it, try logging in again. If you're still having issues after that, please let me know and I'll investigate further." } ], "metadata": { "context": "support_login_issue", "status": "completed", "dataset_ref": "ds_001_entry_001", "turn_count": 6, "date": "2025-01-15T12:45:00Z", "user_profile_extracted": true }, "usage": { "prompt_tokens": 156, "completion_tokens": 234, "total_tokens": 390, "latency": 2.3, "cost_estimate": 0.0012 } } 3.10 Evaluation System Purpose: Automate prompt evaluation using LLM as a "judge" with separate configuration and results.

Evaluation Configuration Schema: { "id": "eval_001", "conversation_id": "conv_001", "prompt_id": "prompt_001", "prompt_version": "v2", "dataset_entry_id": "ds_001_entry_001", "eval_type": "multi-turn", "metrics": { "task_success": 5, "empathy": 4, "logical_guidance": 5, "overall": 4.7 }, "model_config": { "model": "gpt-4o-mini", "temperature": 0.5, "max_tokens": 100, "top_p": 0.9 }, "prompt": "Evaluate the following conversation between a support user and an assistant. Assess based on: Task completion, Tone, and Relevance.Conversation: {conversation} Expected Outcome: Assistant should guide user to recover password politely and efficiently." } Evaluation Results Schema: { "id": "eval_001", "conversation_id": "conv_001", "prompt_id": "prompt_001", "dataset_entry_id": "ds_001_entry_001", "eval_type": "multi-turn", "metrics": { "task_success": 5, "empathy": 4, "logical_guidance": 5, "overall": 4.7 }, "reason": "Assistant provided excellent step-by-step guidance with appropriate empathy. Successfully identified the issue and provided clear resolution path.", "timestamp": "2025-01-15T12:50:00Z", "cost": { "eval_tokens": 89, "cost_estimate": 0.0003 } } Evaluation Prompt Template: You are an evaluator. Evaluate the following conversation between a support user and an assistant. Assess based on: Task completion, Tone, and Relevance.

Conversation: {conversation} Expected Outcome: {expected_outcome}

Return JSON: { "task_success": 4, "tone": 5, "relevance": 4, "overall": 4.3, "reason": "Assistant responded with correct steps and polite tone." } 3.6 Conversation Style Extractor Purpose: Extract how the user speaks and reuse it for simulations.

Extraction Prompt Template Analyze the user's messages and summarize their communication style. Output in JSON: { "tone": "...", "formality": "...", "vocabulary": "...", "sentence_length": "...", "common_patterns": ["..."], "summary": "..." } Output Example { "tone": "polite", "formality": "semi-formal", "vocabulary": "simple", "sentence_length": "short", "common_patterns": ["Could you please", "I think"], "summary": "User communicates politely and clearly with direct phrasing." } Reuse During multi-conversation tests, this style profile can be used to generate synthetic user messages:

You are a simulated user with this style: {{style_profile_json}} Engage in a 4-turn conversation about 'refund policy'. 3.7 Dashboard and Analytics Purpose: Visualize performance, trends, and cost metrics.

Metrics Displayed: Average score per prompt version Per-criterion scores (bar chart) Score evolution across versions (line chart) Token cost per evaluation Model performance comparisons 3.8 Export / Import Purpose: Allow users to save all work locally or share datasets.

Features: Export complete project as JSON Import project to restore Conflict resolution by IDs Example Export File:

{ "project_name": "PromptEval Session", "prompts": [...], "datasets": [...], "eval_results": [...], "user_profiles": [...] } 4. üîÑ AutoEval Engine Logic (Detailed Flow) Single-turn:

Input from dataset ‚Üí send to Model A Receive response Pass response + dataset criteria ‚Üí Model B (evaluator) Parse score JSON ‚Üí store Multi-turn:

Load conversation seed Sequentially send user messages ‚Üí Model A Collect full transcript Pass transcript + criteria ‚Üí Model B Parse score JSON ‚Üí store Batch Mode:

Iterate dataset entries Run above pipeline per entry Aggregate scores by prompt version 5. üíæ Unified Data Schema { "prompts": [ { "id": "prompt_001", "name": "Customer Support Assistant", "folder": "customer-support", "description": "AI assistant for handling customer support queries", "created_at": "2025-01-15T10:15:00Z", "updated_at": "2025-01-15T15:30:00Z", "versions": { "v1": { "version_id": "v1", "text": "You are a helpful customer support assistant. Please respond to user queries with empathy and provide clear solutions.", "config": { "temperature": 0.7, "max_tokens": 500, "top_p": 0.9, "system_prompt": "You are a professional customer support agent for TechCorp." }, "created_at": "2025-01-15T10:15:00Z", "scores": { "avg_score": 4.2, "accuracy": 4.5, "empathy": 4.0, "clarity": 4.1 } } } } ], "datasets": [ { "id": "ds_001", "name": "Customer Support - Login Issues", "type": "multi-turn", "folder": "customer-support", "description": "Multi-turn conversations for login-related customer support scenarios", "created_at": "2025-01-15T10:30:00Z", "tags": ["login", "support", "multi-turn"], "entries": [...] } ], "conversations": [...], "evaluations": [...], "eval_results": [...], "playground_sessions": [...], "settings": { "dataset_generator_config": { "model": "gpt-4o-mini", "temperature": 0.5, "max_tokens": 100, "top_p": 0.9, "prompt": "You are analyzing a user‚Äìassistant conversation. Extract the following information: 1. User's communication style (tone, verbosity, emotion) 2. Conversational goal (task or intent) 3. Criteria for success (how to know assistant did a good job) 4. Example dialogue summary Return JSON:" }, "default_model": { "model": "gpt-4o-mini", "temperature": 0.5, "max_tokens": 100, "top_p": 0.9 } } } 6. üß† Implementation Considerations Concern Strategy LLM key handling Store API keys in localStorage (never uploaded). Rate limits Add configurable delay and retry mechanism. Token cost estimation Estimate based on token counts √ó model pricing table. Parallel evaluations Use Promise.allSettled() with throttling. Error handling Graceful JSON parse errors, retry failed evals. Offline support PWA mode with cached UI and persisted data. 7. üßÆ Example User Workflow Create new project ‚Üí define prompt v1. Test in playground ‚Üí save version. Build dataset of test inputs or conversations. Run AutoEval ‚Äî LLM-vs-LLM scoring. View dashboard ‚Üí identify weak points. Create prompt v2 ‚Üí re-run eval ‚Üí compare. Export results as JSON for sharing. 8. üí° UX Enhancements Feature Enhancement üîç Search Everywhere Add fuzzy search by tag, prompt, or dataset name üß≠ Breadcrumbs Include clickable navigation history ‚åõ Auto-Save Reminder Add top-right indicator + manual save shortcut (Ctrl+S) üìä Quick Compare Mode Version diff viewer: highlight changes in system/user prompt üßæ Inline JSON Preview Expandable raw JSON drawer under each section üß† AI Helper (meta) Optional "Explain This Eval" button powered by evaluator model üíæ Change Log Drawer Track all modifications (prompt v1 ‚Üí v2) Global Layout Features: Sidebar Navigation:

Add hover tooltips and collapsible sidebar for narrow screens Add active page indicator and subtle color-coded icons Top Bar:

Show current session name ("Prompt Eval Session #3") Include theme toggle (light/dark) "Unsaved changes" badge ‚Üí turn into clickable diff viewer (shows what changed since last export) Organization Features: Folder and Tagging System: Organize prompts, datasets, and eval runs by projects or topics Filtering: Filter by folder, tags, type, and other metadata Grouping: Group related items for better organization 9. üìä Future Enhancements (Post-MVP) Area Upgrade Multi-judge scoring Average across multiple evaluators. Per-turn scoring Evaluate each assistant turn individually. LLM cost profiler Compute $ cost per experiment. Collaborative mode Shared JSON import/export merge. Custom eval rubric builder UI-based eval prompt editor. Embedding-based comparison Similarity check between outputs and expected results. ‚úÖ MVP Feature Checklist Category Must-Have Dashboard Prompt summary table, quick actions, auto-save indicator Prompts Page Editor, Playground, Auto Chat, Eval Results tabs Eval Page Config, Run, Results tabs with progress tracking Dataset Page Single/Multi-turn entries with chat-style UI Multi-Chat Page LLM-vs-LLM conversation simulator lab Settings Page General, Dataset Generator, Evaluator, Storage tabs Organization Folder and tagging system for all entities Storage IndexedDB persistence + JSON export/import UX Features Search, breadcrumbs, auto-save, diff viewer sample JSON { "prompts": [ { "id": "prompt_001", "name": "Customer Support Assistant", "folder": "customer-support", "description": "AI assistant for handling customer support queries", "created_at": "2025-01-15T10:15:00Z", "updated_at": "2025-01-15T15:30:00Z", "versions": { "v1": { "version_id": "v1", "text": "You are a helpful customer support assistant. Please respond to user queries with empathy and provide clear solutions.", "config": { "temperature": 0.7, "max_tokens": 500, "top_p": 0.9, "system_prompt": "You are a professional customer support agent for TechCorp." }, "created_at": "2025-01-15T10:15:00Z", "scores": { "avg_score": 4.2, "accuracy": 4.5, "empathy": 4.0, "clarity": 4.1 } }, "v2": { "version_id": "v2", "text": "You are a helpful customer support assistant. Please respond to user queries with empathy, acknowledge their concerns, and provide step-by-step solutions. Always ask clarifying questions when needed.", "config": { "temperature": 0.6, "max_tokens": 600, "top_p": 0.8, "system_prompt": "You are a professional customer support agent for TechCorp. Always be empathetic and solution-oriented." }, "created_at": "2025-01-15T11:30:00Z", "scores": { "avg_score": 4.6, "accuracy": 4.8, "empathy": 4.7, "clarity": 4.5 } }, "v3": { "version_id": "v3", "text": "You are a helpful customer support assistant. Please respond to user queries with empathy, acknowledge their concerns, and provide step-by-step solutions. Always ask clarifying questions when needed. If you cannot resolve the issue, escalate to a human agent.", "config": { "temperature": 0.5, "max_tokens": 700, "top_p": 0.7, "system_prompt": "You are a professional customer support agent for TechCorp. Always be empathetic, solution-oriented, and know when to escalate." }, "created_at": "2025-01-15T14:45:00Z", "scores": { "avg_score": 4.8, "accuracy": 4.9, "empathy": 4.8, "clarity": 4.7 } } } }, { "id": "prompt_002", "name": "Technical Documentation Assistant", "folder": "technical", "description": "AI assistant for technical documentation and API help", "created_at": "2025-01-15T12:00:00Z", "updated_at": "2025-01-15T15:00:00Z", "versions": { "v1": { "version_id": "v1", "text": "You are a technical documentation assistant. Provide clear, accurate technical information and code examples.", "config": { "temperature": 0.3, "max_tokens": 800, "top_p": 0.9, "system_prompt": "You are a technical documentation expert. Provide accurate, well-formatted technical information." }, "created_at": "2025-01-15T12:00:00Z", "scores": { "avg_score": 4.4, "accuracy": 4.7, "technical_precision": 4.5, "clarity": 4.2 } } } } ], "datasets": [ { "id": "ds_001", "name": "Customer Support - Login Issues", "type": "multi-turn", "folder": "customer-support", "description": "Multi-turn conversations for login-related customer support scenarios", "created_at": "2025-01-15T10:30:00Z", "tags": ["login", "support", "multi-turn"], "entries": [ { "id": "ds_001_entry_001", "type": "multi-turn", "title": "Password Reset Scenario", "system_context": "The assistant is a helpful tech support agent for a SaaS product.", "user_behavior": { "style": "Conversational, frustrated but polite", "formality": "semi-formal", "goal": "user forget his password and assistant help him to reset his password his email is test@test.com", "data": { "email": "test@test.com", "name": "John Doe" } }, "conversation": [ { "role": "user", "content": "Hi, I'm having trouble logging in." } ], "created_at": "2025-01-15T10:30:00Z" } ] }, { "id": "ds_002", "name": "Single-Turn Summarization", "type": "single-turn", "folder": "content", "description": "Single-turn prompts for content summarization tasks", "created_at": "2025-01-15T11:00:00Z", "tags": ["summarization", "single-turn", "content"], "entries": [ { "id": "ds_002_entry_001", "type": "single-turn", "input": "Explain the concept of inflation in simple terms for a general audience.", "expected_behavior": "Clear, concise explanation using simple language and relatable examples.", "created_at": "2025-01-15T11:00:00Z" }, { "id": "ds_002_entry_002", "type": "single-turn", "input": "Summarize the key points of this technical documentation about API authentication.", "expected_behavior": "Accurate technical summary highlighting main concepts and implementation steps.", "created_at": "2025-01-15T11:15:00Z" } ] } ], "conversations": [ { "id": "conv_001", "prompt_id": "prompt_001", "prompt_version": "v2", "model": "gpt-4o-mini", "type": "manual", "messages": [ { "role": "user", "content": "Hi, I'm having trouble logging in." }, { "role": "assistant", "content": "I'm sorry to hear you're having trouble logging in. I'd be happy to help you resolve this. What specific error message are you seeing when you try to log in?" }, { "role": "user", "content": "It says 'invalid credentials' but I know my password is correct." }, { "role": "assistant", "content": "I understand your frustration. Let's troubleshoot this step by step. First, could you please try resetting your password? I can send you a password reset link to your registered email address. What email address do you have on file with us?" }, { "role": "user", "content": "my email is john.doe@example.com" }, { "role": "assistant", "content": "Perfect! I've sent a password reset link to john.doe@example.com. Please check your email (including spam folder) and click the link to reset your password. Once you've reset it, try logging in again. If you're still having issues after that, please let me know and I'll investigate further." } ], "metadata": { "context": "support_login_issue", "status": "completed", "dataset_ref": "ds_001_entry_001", "turn_count": 6, "date": "2025-01-15T12:45:00Z", "user_profile_extracted": true }, "usage": { "prompt_tokens": 156, "completion_tokens": 234, "total_tokens": 390, "latency": 2.3, "cost_estimate": 0.0012 } }, { "id": "conv_002", "prompt_id": "prompt_001", "prompt_version": "v2", "model": "gpt-4o-mini", "type": "auto_eval", "messages": [ { "role": "user", "content": "URGENT: My account is locked and I have a meeting in 10 minutes!" }, { "role": "assistant", "content": "I understand this is urgent and I'll help you get back into your account quickly. Let me check your account status right away. Can you tell me your username or email address so I can look up your account?" }, { "role": "user", "content": "username: jsmith, email: jsmith@company.com" }, { "role": "assistant", "content": "I can see your account is temporarily locked due to multiple failed login attempts. I'm unlocking it now and resetting your password. You should receive a new temporary password via email in the next 2 minutes. Use that to log in immediately, and you can change it to something more memorable after your meeting." }, { "role": "user", "content": "Thank you! I got the email. Will this work for my mobile app too?" }, { "role": "assistant", "content": "Yes, the temporary password will work for both web and mobile app. You're all set for your meeting! After your meeting, I recommend changing to a strong password and enabling two-factor authentication to prevent this from happening again." } ], "metadata": { "context": "urgent_account_unlock", "status": "completed", "dataset_ref": "ds_001_entry_002", "turn_count": 6, "date": "2025-01-15T13:15:00Z", "simulated_user": true }, "usage": { "prompt_tokens": 189, "completion_tokens": 267, "total_tokens": 456, "latency": 2.8, "cost_estimate": 0.0014 } } ], "evaluations": [ { "id": "eval_001", "conversation_id": "conv_001", "prompt_id": "prompt_001", "prompt_version": "v2", "dataset_entry_id": "ds_001_entry_001", "eval_type": "multi-turn", "metrics": { "task_success": 5, "empathy": 4, "logical_guidance": 5, "overall": 4.7 }, "model_config": { "model": "gpt-4o-mini", "temperature": 0.5, "max_tokens": 100, "top_p": 0.9 }, "prompt": "Evaluate the following conversation between a support user and an assistant. Assess based on: Task completion, Tone, and Relevance.Conversation: {conversation} Expected Outcome: Assistant should guide user to recover password politely and efficiently." } ], "eval_results": [ { "id": "eval_001", "conversation_id": "conv_001", "prompt_id": "prompt_001", "dataset_entry_id": "ds_001_entry_001", "eval_type": "multi-turn", "metrics": { "task_success": 5, "empathy": 4, "logical_guidance": 5, "overall": 4.7 }, "reason": "Assistant provided excellent step-by-step guidance with appropriate empathy. Successfully identified the issue and provided clear resolution path.", "timestamp": "2025-01-15T12:50:00Z", "cost": { "eval_tokens": 89, "cost_estimate": 0.0003 } } ], "playground_sessions": [ { "id": "playground_001", "prompt_id": "prompt_001", "prompt_version": "v3", "model": "gpt-4o-mini", "config": { "temperature": 0.5, "max_tokens": 700, "top_p": 0.7 }, "messages": [ { "role": "user", "content": "I need help with my subscription billing" }, { "role": "assistant", "content": "I'd be happy to help you with your subscription billing. What specific issue are you experiencing? Are you having trouble with payment, need to change your plan, or have questions about charges?" } ], "usage": { "prompt_tokens": 45, "completion_tokens": 67, "total_tokens": 112, "latency": 1.8, "cost_estimate": 0.0003 }, "timestamp": "2025-01-15T15:00:00Z", "saved_as_version": false, "saved_as_dataset": false } ], "export_metadata": { "exported_at": "2025-01-15T15:30:00Z", "export_version": "1.0.0", "total_prompts": 2, "total_conversations": 2, "total_evaluations": 3, "total_datasets": 2, "total_user_profiles": 2, "file_size_bytes": 15420 }, "settings": { "dataset_generator_config": { "model": "gpt-4o-mini", "temperature": 0.5, "max_tokens": 100, "top_p": 0.9, "prompt": "You are analyzing a user‚Äìassistant conversation. Extract the following information: 1. User's communication style (tone, verbosity, emotion) 2. Conversational goal (task or intent) 3. Criteria for success (how to know assistant did a good job) 4. Example dialogue summary Return JSON:" }, "default_model": { "model": "gpt-4o-mini", "temperature": 0.5, "max_tokens": 100, "top_p": 0.9 } } }